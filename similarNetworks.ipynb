{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similar Network Analytics ....\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import renders as rs\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import renders as rs\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "#get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "\n",
    "def load() :\n",
    "\n",
    "    print \"............................ Inside Data Loading .......................................................\"\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data = pd.read_csv(\"customers.csv\")\n",
    "        temp = pd.read_csv(\"customers.csv\")\n",
    "        data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "        temp.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "        print \"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape)\n",
    "    except:\n",
    "        print \"Dataset could not be loaded. Is the dataset missing?\"\n",
    "\n",
    "    return data, temp\n",
    "\n",
    "def explore(data) :\n",
    "\n",
    "    print \"............................Inside Data Exploration .......................................................\"\n",
    "\n",
    "    # Display a description of the dataset\n",
    "    display(data.describe())\n",
    "\n",
    "    #Choosing a temp variable just to get the 'total' of each individual customers\n",
    "\n",
    "    temp['total'] = temp.sum(axis=1)\n",
    "\n",
    "    display(temp.describe())\n",
    "\n",
    "    print temp[:10]\n",
    "\n",
    "def selectSample() :\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    indices.append(92)\n",
    "    indices.append(200)\n",
    "    indices.append(150)\n",
    "\n",
    "    # Create a DataFrame of the chosen samples\n",
    "    samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\n",
    "    print \"Chosen samples of wholesale customers dataset:\"\n",
    "    display(samples)\n",
    "\n",
    "    # Create a DataFrame of the chosen samples\n",
    "    samples1 = pd.DataFrame(temp.loc[indices], columns = temp.keys()).reset_index(drop = True)\n",
    "    print \"Chosen samples of wholesale customers dataset with totals:\"\n",
    "    display(samples1)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def featureRelevance(data) :\n",
    "\n",
    "    print \"............................Inside Feature Relevance .......................................................\"\n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn import cross_validation\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    # TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "\n",
    "    target_cols = data.columns[-2]\n",
    "    print target_cols\n",
    "    y = data[target_cols]\n",
    "\n",
    "    new_data = data.copy(deep=True)\n",
    "\n",
    "    # Removing Milk as it is most correlated with others, it might turns out to be good target\n",
    "\n",
    "    new_data.drop(['Detergents_Paper'], axis = 1, inplace = True)\n",
    "\n",
    "    X = new_data\n",
    "\n",
    "    seed = 7\n",
    "    t_size = 0.25\n",
    "\n",
    "\n",
    "    # TODO: Split the data into training and testing sets using the given feature as the target\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y,\n",
    "      test_size=t_size, random_state=seed)\n",
    "\n",
    "\n",
    "    # TODO: Create a decision tree regressor and fit it to the training set\n",
    "    regressor = DecisionTreeRegressor(random_state = seed)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # TODO: Report the score of the prediction using the testing set\n",
    "    predictions = regressor.predict(X_test)\n",
    "    score = regressor.score(X_test, y_test)\n",
    "    print score\n",
    "\n",
    "def featureDistributions(data) :\n",
    "\n",
    "    print \"............................Inside Feature Distributions .......................................................\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Produce a scatter matrix for each pair of features in the data\n",
    "    pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n",
    "\n",
    "    # density\n",
    "    data.plot(kind='density', subplots=True, layout=(3,2), sharex=False, legend=False,\n",
    "    fontsize= 1)\n",
    "    plt.show()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    names = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper' ,'Delicatessen' ]\n",
    "\n",
    "    # correlation\n",
    "\n",
    "    corr = data.corr()\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    #with sns.axes_style(\"white\"):\n",
    "    #    ax = sns.heatmap(corr, mask=mask, square=True, annot=True, cmap='RdBu')\n",
    "    #    plt.xticks(rotation=45, ha='center');\n",
    "\n",
    "    print \"............................End Feature Distributions .......................................................\"\n",
    "\n",
    "def normalizeFeatures(data,samples) :\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print \"............................ Inside Normalize / Feature scalaing using Log .......................................................\"\n",
    "\n",
    "    normalizedData = np.log(data)\n",
    "\n",
    "    log_samples = np.log(samples)\n",
    "\n",
    "    # Produce a scatter matrix for each pair of newly-transformed features\n",
    "    pd.scatter_matrix(normalizedData, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n",
    "\n",
    "    #reviewer suggested to use log_data.boxplot(); but it didn't work well so I keeping my original.\n",
    "\n",
    "    # box and whisker plots\n",
    "    normalizedData.plot(kind='box', subplots=True, layout=(3,2), sharex=False, sharey=False,\n",
    "    fontsize=8)\n",
    "    plt.show()\n",
    "    display(log_samples)\n",
    "\n",
    "    print \"............................ End Normalize / Feature scalaing using Log .......................................................\"\n",
    "    return normalizedData, log_samples\n",
    "\n",
    "def removeOutliers(normalizedData) :\n",
    "\n",
    "    print \"............................Outliers removal .......................................................\"\n",
    "\n",
    "    from scipy import stats\n",
    "    #from  more_itertools import unique_everseen\n",
    "\n",
    "    # Keep outlier indices in a list and examine after looping thru the features\n",
    "    idx = []\n",
    "\n",
    "\n",
    "    # For each feature find the data points with extreme high or low values\n",
    "    for feature in normalizedData.keys():\n",
    "\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(normalizedData[feature], 25)\n",
    "\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(normalizedData[feature], 75)\n",
    "\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "\n",
    "        # Display the outliers\n",
    "        print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "        display(normalizedData[~((normalizedData[feature] >= Q1 - step) & (normalizedData[feature] <= Q3 + step))])\n",
    "\n",
    "        # Gather the indexes of all the outliers\n",
    "        idx += normalizedData[~((normalizedData[feature] >= Q1 - step) & (normalizedData[feature] <= Q3 + step))].index.tolist()\n",
    "\n",
    "    print(sorted(idx))\n",
    "\n",
    "    # OPTIONAL: Select the indices for data points you wish to remove\n",
    "    outliers  = []\n",
    "\n",
    "    #outliers = list(unique_everseen(idx))\n",
    "\n",
    "    import collections\n",
    "    outliers = [item for item, count in collections.Counter(idx).items() if count > 1]\n",
    "\n",
    "\n",
    "    print(sorted(outliers))\n",
    "\n",
    "    # Remove the outliers, if any were specified\n",
    "    good_data = normalizedData.drop(normalizedData.index[outliers]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    print(normalizedData.shape)\n",
    "    print(good_data.shape)\n",
    "\n",
    "    print \"............................End Outliers removal .......................................................\"\n",
    "\n",
    "    return good_data\n",
    "\n",
    "def performPCA(good_data,log_samples) :\n",
    "\n",
    "    print \"............................PCA .......................................................\"\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # TODO: Apply PCA to the good data with the same number of dimensions as features\n",
    "\n",
    "    pca = PCA(n_components=6).fit(good_data)\n",
    "\n",
    "    # TODO: Apply a PCA transformation to the sample log-data\n",
    "    pca_samples = pca.transform(log_samples)\n",
    "    #pca.fit(log_samples)\n",
    "\n",
    "    # Generate PCA results plot\n",
    "    pca_results = rs.pca_results(good_data, pca)\n",
    "\n",
    "    # cumulative explaned variance\n",
    "    #print '\\n', np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Display sample log-data after having a PCA transformation applied\n",
    "    display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))\n",
    "\n",
    "\n",
    "    # TODO: Fit PCA to the good data using only two dimensions\n",
    "    pca = PCA(n_components=2).fit(good_data)\n",
    "\n",
    "    # TODO: Apply a PCA transformation the good data\n",
    "    reduced_data = pca.transform(good_data)\n",
    "\n",
    "    # TODO: Apply a PCA transformation to the sample log-data\n",
    "    pca_samples = pca.transform(log_samples)\n",
    "\n",
    "    # Create a DataFrame for the reduced data\n",
    "    reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "    display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n",
    "    print reduced_data[:10]\n",
    "\n",
    "    return pca,reduced_data,pca_samples\n",
    "\n",
    "def performClustering(reduced_data,pca_samples) :\n",
    "\n",
    "    print \"............................Inside Clustering .......................................................\"\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    #keep the scores for each cluster size\n",
    "    sil_scores = []\n",
    "\n",
    "    random_state = 7\n",
    "\n",
    "    for i in range(7,1,-1):\n",
    "        clusterer = KMeans(i, random_state=random_state).fit(reduced_data)\n",
    "        # TODO: Predict the cluster for each data point\n",
    "        preds = clusterer.predict(reduced_data)\n",
    "\n",
    "        # TODO: Find the cluster centers\n",
    "        centers = clusterer.cluster_centers_\n",
    "\n",
    "        # TODO: Predict the cluster for each transformed sample data point\n",
    "        sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "        # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "        score = silhouette_score(reduced_data, preds)\n",
    "        sil_scores.append(score)\n",
    "        print i, 'clusters:', score.round(5)\n",
    "\n",
    "    # plot the scores\n",
    "    import matplotlib.pyplot as plt\n",
    "    _ = plt.plot(np.arange(7,1,-1), sil_scores, '-o')\n",
    "\n",
    "    print \"............................End Clustering .......................................................\"\n",
    "\n",
    "    rs.cluster_results(reduced_data, preds, centers, pca_samples)\n",
    "\n",
    "    return preds, centers, pca_samples\n",
    "\n",
    "def identifyCenter(pca,reduced_data, preds, centers, pca_samples) :\n",
    "\n",
    "    print \"............................Identify Centers .......................................................\"\n",
    "\n",
    "    rs.cluster_results(reduced_data, preds, centers, pca_samples)\n",
    "\n",
    "    log_centers = pca.inverse_transform(centers)\n",
    "    true_centers = np.exp(log_centers)\n",
    "\n",
    "    # Display the true centers\n",
    "    segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "\n",
    "    true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\n",
    "    true_centers.index = segments\n",
    "    true_centers['total'] = true_centers.sum(axis=1)\n",
    "\n",
    "    display(true_centers)\n",
    "\n",
    "    #true_centers.plot(kind = 'bar', figsize = (10, 4))\n",
    "\n",
    "def displayPredictons(sample_preds,outliers,pca_samples) :\n",
    "\n",
    "    #Display the predictions\n",
    "    #print sample_preds\n",
    "\n",
    "    for i, pred in enumerate(sample_preds):\n",
    "        print \"Sample point\", i, \"predicted to be in Cluster\", pred\n",
    "\n",
    "    rs.channel_results(reduced_data, outliers, pca_samples)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print \" Similar Network Analytics ....\"\n",
    "    #data, temp = load()\n",
    "    #explore(data)\n",
    "    #samples = selectSample()\n",
    "    #featureRelevance(data)\n",
    "    #featureDistributions(data)\n",
    "    #normalizedData,log_samples = normalizeFeatures(data,samples)\n",
    "    #good_data = removeOutliers(normalizedData)\n",
    "    #pca,reduced_data,pca_samples = performPCA(good_data,log_samples)\n",
    "    #preds, centers, pca_samples = performClustering(reduced_data,pca_samples)\n",
    "    #identifyCenter(pca,reduced_data, preds, centers, pca_samples)\n",
    "    #displayPredictons(sample_preds,outliers,pca_samples)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
