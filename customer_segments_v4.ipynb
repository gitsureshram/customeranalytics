{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Unsupervised Learning\n",
    "## Project 3: Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in *monetary units*) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n",
    "\n",
    "The dataset for this project can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). For the purposes of this project, the features `'Channel'` and `'Region'` will be excluded in the analysis â€” with focus instead on the six product categories recorded for customers.\n",
    "\n",
    "Run the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset could not be loaded. Is the dataset missing?\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import renders as rs\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the wholesale customers dataset\n",
    "try:\n",
    "    data = pd.read_csv(\"export_psirt.csv\")\n",
    "    temp = pd.read_csv(\"export_psirt.csv\")\n",
    "    #data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "    #temp.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "    print \"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape)\n",
    "except:\n",
    "    print \"Dataset could not be loaded. Is the dataset missing?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "In this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project.\n",
    "\n",
    "Run the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: **'Fresh'**, **'Milk'**, **'Grocery'**, **'Frozen'**, **'Detergents_Paper'**, and **'Delicatessen'**. Consider what each category represents in terms of products you could purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a5d341458e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Display a description of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Choosing a temp variable just to get the 'total' of each individual customers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#Reviewer comment: Try to describe the samples in terms of how their category spending relates to the overall category \n",
    "#spending stats in data.describe(). You don't need to create another dataframe temp to do this, just compare the samples' \n",
    "#category spending to the means and/or medians of the product categorie my question to reviewer is that well \n",
    "#you can look at percentile for sample data points but you can't get idea about whole dataset, with total you can do that.\n",
    "\n",
    "# I don't agree with the reviewer comment as it will complicate the initail analysis, 'Total' stats gives us \n",
    "#clean and easy way to understand the objective function so I will stick with temp.\n",
    "\n",
    "\n",
    "# Display a description of the dataset\n",
    "display(data.describe())\n",
    "\n",
    "#Choosing a temp variable just to get the 'total' of each individual customers\n",
    "\n",
    "temp['total'] = temp.sum(axis=1)\n",
    "\n",
    "display(temp.describe())\n",
    "\n",
    "print temp[:10]\n",
    "#display(temp.describe())\n",
    "\n",
    "#Look at the stats for 'total' column it suggests us the individual spending profile\n",
    "# if we catagoried this in groups these are some groups appears\n",
    "#Group 1 (G1) Data points near quartile 25% 17448\n",
    "#Group 2 (G2) data points near quartile 50% 27492\n",
    "#Group 3 (G3) data points near quartile 75% 41307\n",
    "#Group 4 (G4) data points near min 904\n",
    "#Group 5 (G5) data points near max 199891\n",
    "#Group 6 (G6) data points near mean 33226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Selecting Samples\n",
    "To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add **three** indices of your choice to the `indices` list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Select three indices of your choice you wish to sample from the dataset\n",
    "indices = []\n",
    "\n",
    "indices.append(92)\n",
    "indices.append(200)\n",
    "indices.append(150)\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\n",
    "print \"Chosen samples of wholesale customers dataset:\"\n",
    "display(samples)\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples1 = pd.DataFrame(temp.loc[indices], columns = temp.keys()).reset_index(drop = True)\n",
    "print \"Chosen samples of wholesale customers dataset with totals:\"\n",
    "display(samples1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Consider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers.  \n",
    "*What kind of establishment (customer) could each of the three samples you've chosen represent?*  \n",
    "**Hint:** Examples of establishments include places like markets, cafes, and retailers, among many others. Avoid using names for establishments, such as saying *\"McDonalds\"* when describing a sample customer as a restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in question is the yearly suppy of products for each customers. From descriptive stats we can see:\n",
    "\n",
    "From despriptive stat for field 'total' we can see 6 clear groups (G1, G2, G3, G4, G5 and G6). Please see my comments earier on the groups.\n",
    "\n",
    "Please note (Q stands for quartile and G stands for groups), also remember from descritive stats that the average total spending of customers is 33226.\n",
    "\n",
    "G1 ( for field 'total' Q 25% is 17448) --> Represent very low yearly total expenditure, probably small caffe or resturant spending less than total average.\n",
    "\n",
    "G2 (for field 'total' Q 50% is 27492)  --> In between of Q 25% and Q 50% but still spending less than 'total' average\n",
    "\n",
    "G3 (for field 'total' Q 75% is 41307) --> Represent high yearly expenditure, pobably big caffe chains, resutant chaines, retailers, markets, spending above average\n",
    "\n",
    "G4 (for field 'total near minimum 904) --> Data points near the minimum, these are not year long consistent buyers it suggest they might be getting their stuff from others or they are really small buyers, this group must pay attention as this present the opportunity to grow business in this group.\n",
    "\n",
    "G5 (for field 'total' near maximum 199891) ---> These data points indicates the biggest customers as well as biggest enterprises like big retailers chains.\n",
    "\n",
    "G6 (for field 'total' near Mean 33226) --> The data points near the average and we can consider these as the year long medium buyer and also suggest medium size customers.\n",
    "\n",
    "As per this explanation\n",
    "\n",
    "0 represents in G5 maximum category and this might be big enterprises like big retailers chains or large resturant chain\n",
    "1 represents in G3 big caffe, retailer or resturant\n",
    "2 represents in G1, low expenditure, probably small caffe or resturant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Feature Relevance\n",
    "One interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign `new_data` a copy of the data by removing a feature of your choice using the `DataFrame.drop` function.\n",
    " - Use `sklearn.cross_validation.train_test_split` to split the dataset into training and testing sets.\n",
    "   - Use the removed feature as your target label. Set a `test_size` of `0.25` and set a `random_state`.\n",
    " - Import a decision tree regressor, set a `random_state`, and fit the learner to the training data.\n",
    " - Report the prediction score of the testing set using the regressor's `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db27de79ed84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtarget_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtarget_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "\n",
    "target_cols = data.columns[-2] \n",
    "print target_cols\n",
    "y = data[target_cols]\n",
    "\n",
    "new_data = data.copy(deep=True)\n",
    "\n",
    "# Removing Milk as it is most correlated with others, it might turns out to be good target \n",
    "\n",
    "#new_data.drop(['Detergents_Paper'], axis = 1, inplace = True)\n",
    "\n",
    "X = new_data\n",
    "\n",
    "seed = 7\n",
    "t_size = 0.25\n",
    "\n",
    "\n",
    "# TODO: Split the data into training and testing sets using the given feature as the target\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y,\n",
    "  test_size=t_size, random_state=seed)\n",
    "                                                                     \n",
    "\n",
    "# TODO: Create a decision tree regressor and fit it to the training set\n",
    "regressor = DecisionTreeRegressor(random_state = seed)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "predictions = regressor.predict(X_test)\n",
    "score = regressor.score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "*Which feature did you attempt to predict? What was the reported prediction score? Is this feature is necessary for identifying customers' spending habits?*  \n",
    "**Hint:** The coefficient of determination, `R^2`, is scored between 0 and 1, with 1 being a perfect fit. A negative `R^2` implies the model fails to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review code note:\n",
    "You should also set a random seed with your DecisionTreeRegressor()\n",
    "    Shekhar:I guess I have already set a random seed in the cross validation which whould effect the DecisionTreeRegressor()\n",
    "We can also more easily get the score with regressor.score(X_test, y_test)\n",
    "    Shekhar: Agreed and implemented\n",
    "    \n",
    " \n",
    "I would like to answer this in 2 parts\n",
    "1) I tried to all 6 variables as a target, here is the r2 score I got\n",
    "Milk as a target = -2.3\n",
    "Fresh as a target = -0.45\n",
    "Grocery as a target = 0.53\n",
    "Frozen as a target = -1.14\n",
    "Detergents_Paper as a target = 0.69\n",
    "Delicatessenas as a target = -1.99\n",
    "\n",
    "I agree with review comments Detergents_Paper and Grocery can be predicted from other feature(high correlation), these features would actually not be good for identifying customers' spending habits, as when a customer purchases one of these they also purchase another, therefore not good for identifying customers' spending habits.\n",
    "\n",
    "2) I created another column named'total' which is the sum of all products for individual customers and when I tried to predict total it gave the r2 score of 0.83 and shows it able to predict data much better that individual variables\n",
    "The implemenation of Total can be seen in another notebook (attached), if you have a time then please have a look\n",
    "\n",
    "Overall I feel the column 'Total' is much better predictor of the spending habits of customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Distributions\n",
    "To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ef5b9238249c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Produce a scatter matrix for each pair of features in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'kde'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# density\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Produce a scatter matrix for each pair of features in the data\n",
    "pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n",
    "\n",
    "# density\n",
    "data.plot(kind='density', subplots=True, layout=(3,2), sharex=False, legend=False,\n",
    "fontsize= 1)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "names = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper' ,'Delicatessen' ]\n",
    "\n",
    "# correlation\n",
    "\n",
    "corr = data.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True, annot=True, cmap='RdBu')\n",
    "    plt.xticks(rotation=45, ha='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "*Are there any pairs of features which exhibit some degree of correlation? Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? How is the data for those features distributed?*  \n",
    "**Hint:** Is the data normally distributed? Where do most of the data points lie? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any pairs of features which exhibit some degree of correlation?\n",
    "- If you look at the pearson co-relation matrix and plot\n",
    "Milk is correlated with Grocery (0.73) and Detergents_Paper(0.66)\n",
    "Grocery is highly correlated with Detergents_Paper (0.92)\n",
    "\n",
    "Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict?\n",
    "Out of these Milk (r2 score = -2.3) is not a good predictor but Grocery (r2 score = 0.53) is better predictor target.\n",
    "Also you can see that Detergents_Paper(r2 score = 0.69) is best correlated with Milk and Grocery with other so it is the best predictor as a target\n",
    "\n",
    "How is the data for those features distributed?\n",
    "From the density plot we can see that all 6 features are not normally distributed, they are positively skewed. Probably it need normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Feature Scaling\n",
    "If data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics) to apply a non-linear scaling â€” particularly for financial data. One way to achieve this scaling is by using a [Box-Cox test](http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html), which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign a copy of the data to `log_data` after applying a logarithm scaling. Use the `np.log` function for this.\n",
    " - Assign a copy of the sample data to `log_samples` after applying a logrithm scaling. Again, use `np.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a141dbc69615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Scale the data using the natural logarithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: Scale the sample data using the natural logarithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Scale the data using the natural logarithm\n",
    "\n",
    "log_data = np.log(data) \n",
    "\n",
    "# TODO: Scale the sample data using the natural logarithm\n",
    "log_samples = np.log(samples)\n",
    "\n",
    "# Produce a scatter matrix for each pair of newly-transformed features\n",
    "pd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n",
    "\n",
    "#reviewer suggested to use log_data.boxplot(); but it didn't work well so I keeping my original.\n",
    "\n",
    "# box and whisker plots\n",
    "log_data.plot(kind='box', subplots=True, layout=(3,2), sharex=False, sharey=False,\n",
    "fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "After applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).\n",
    "\n",
    "Run the code below to see how the sample data has changed after having the natural logarithm applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c1509005a0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display the log-transformed sample data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the log-transformed sample data\n",
    "display(log_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Outlier Detection\n",
    "Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign the value of the 25th percentile for the given feature to `Q1`. Use `np.percentile` for this.\n",
    " - Assign the value of the 75th percentile for the given feature to `Q3`. Again, use `np.percentile`.\n",
    " - Assign the calculation of an outlier step for the given feature to `step`.\n",
    " - Optionally remove data points from the dataset by adding indices to the `outliers` list.\n",
    "\n",
    "**NOTE:** If you choose to remove any outliers, ensure that the sample data does not contain any of these points!  \n",
    "Once you have performed this implementation, the dataset will be stored in the variable `good_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named more_itertools",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0c8bf6eb8989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mmore_itertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_everseen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Keep outlier indices in a list and examine after looping thru the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named more_itertools"
     ]
    }
   ],
   "source": [
    "# I must say this idea is good from reviewer, I now throughly understand the counters assignments using index.\n",
    "\n",
    "from scipy import stats\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "# Keep outlier indices in a list and examine after looping thru the features\n",
    "idx = []\n",
    "\n",
    "\n",
    "# For each feature find the data points with extreme high or low values\n",
    "for feature in log_data.keys():\n",
    "    \n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(log_data[feature], 25)\n",
    "    \n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(log_data[feature], 75)\n",
    "    \n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = 1.5*(Q3 - Q1)\n",
    "    \n",
    "    # Display the outliers\n",
    "    print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n",
    "    \n",
    "    # Gather the indexes of all the outliers\n",
    "    idx += log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index.tolist()\n",
    "\n",
    "print(sorted(idx))\n",
    " \n",
    "# OPTIONAL: Select the indices for data points you wish to remove\n",
    "outliers  = []\n",
    "\n",
    "#outliers = list(unique_everseen(idx))\n",
    "\n",
    "import collections\n",
    "\n",
    "outliers = [item for item, count in collections.Counter(idx).items() if count > 1]\n",
    "\n",
    "\n",
    "print(sorted(outliers))\n",
    "\n",
    "# Remove the outliers, if any were specified\n",
    "good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "print(log_data.shape)\n",
    "print(good_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4\n",
    "*Are there any data points considered outliers for more than one feature? Should these data points be removed from the dataset? If any data points were added to the `outliers` list to be removed, explain why.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Are there any data points considered outliers for more than one feature? Should these data points be removed from the dataset\n",
    "\n",
    "An outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.\n",
    "\n",
    "Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model.\n",
    "\n",
    "- Also as per the box plot (above) and method IQR shows there are many outliers \n",
    "As per IQR \n",
    "Find the inter quartile range, which is IQR = Q3 - Q1, where Q3 is the third quartile and Q1 is the first quartile. Then find these two numbers: a) Q1 - 1.5*IQR b) Q3 + 1.5*IQR The point is an outlier if < a or > b\n",
    "\n",
    "    If any data points were added to the outliers list to be removed, explain why.\n",
    "\n",
    "We can see that with this method 1/2 of the points are considered as outliers and I don't like to remove so many data points so I have implemented another method Z score (based on standard deviation), I have considered the data outside the 2sigma as outliers  this will remove 62 outlier points from dataset.\n",
    "\n",
    "we have to sometime careful as Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "In this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implementation: PCA\n",
    "\n",
    "Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the *explained variance ratio* of each dimension â€” how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Import `sklearn.decomposition.PCA` and assign the results of fitting PCA in six dimensions with `good_data` to `pca`.\n",
    " - Apply a PCA transformation of the sample log-data `log_samples` using `pca.transform`, and assign the results to `pca_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fcc3b6c3732f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TODO: Apply PCA to the good data with the same number of dimensions as features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO: Apply a PCA transformation to the sample log-data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'good_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# TODO: Apply PCA to the good data with the same number of dimensions as features\n",
    "\n",
    "pca = PCA(n_components=6).fit(good_data)\n",
    "\n",
    "# TODO: Apply a PCA transformation to the sample log-data\n",
    "pca_samples = pca.transform(log_samples)\n",
    "#pca.fit(log_samples)\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = rs.pca_results(good_data, pca)\n",
    "\n",
    "# cumulative explaned variance\n",
    "#print '\\n', np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 5\n",
    "*How much variance in the data is explained* ***in total*** *by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending.*  \n",
    "**Hint:** A positive increase in a specific dimension corresponds with an *increase* of the *positive-weighted* features and a *decrease* of the *negative-weighted* features. The rate of increase or decrease is based on the indivdual feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How much variance in the data is explained in total by the first and second principal component?  What about the first four principal components\n",
    "With first it is 0.44\n",
    "with second it is 0.26\n",
    "After that explained variance is declining sharply from 0.1 (3rd) to 0.02 (6th)\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. Usally the number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible, we can see that in our case the first component is explaining 0.44), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. \n",
    "\n",
    "\n",
    "    Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending.\n",
    "\n",
    "PCA orders its output in terms of explained variance, i.e. the fraction of variance in the data explained by the relationships encoded in each principal component. So we know that the first pricipal component explains more of the variance than any other, the second component more than any other but the first and so on down the line. In our example, the first two principal components account for 44.0% and 26.0% of the variance in the data respectively. In contrast, the third principal component accounts for only 12.0% of the variance. It is clear from the plot above that the explained variance drops off rapidly after the first two components\n",
    "\n",
    "        Another important point to note Interpretation of the principal components is based on finding which variables are most strongly correlated with each component, i.e., which of these numbers are large in magnitude, the farthest from zero in either positive or negative direction. Which numbers we consider to be large or small is of course is a subjective decision. You need to determine at what level the correlation value will be of importance. Here a correlation value above 0.5 is deemed important\n",
    "\n",
    "The first principal component dimension has a coefficient of +0.78 for the 'Detergents_Paper' feature, +0.42 for the 'grocery' feature and so on. It's pretty obvious that the first principal component is predominantly the 'Detergents_Paper' followed by grocery and Milk feature. It explained the variance of about 44%\n",
    "\n",
    "The second principal component dimension has a coefficient of +0.70 for the 'Fresh' feature' +0.50 for Frozen and 'Delicatessen' feature and so on. Second principal component is predominantly the 'Fresh', Frozen and  'Delicatessen'. It explained the variance of about 26%\n",
    "\n",
    "The third principal component dimension has a coefficient of +0.65 for the 'Delicatessen' feature and -0.65 for the 'Fresh' feature and so on. It's pretty obvious that the third principal component is predominantly the 'Delicatessen'. It explained the variance of about 12%\n",
    "\n",
    "The fourth principal component dimension has a coefficient of +0.80 for the 'Frozen' feature and -0.58 for Delicatessen . It's pretty obvious that the fourth principal component is predominantly the 'Frozen'. It explained the variance of about 10%\n",
    "\n",
    "    Also we can note that the cumulative sum of explained variance, with the help of first 2 components we are able to explain 70% of variance and with the first 4 components 93% of the variance.\n",
    "\n",
    "based on these 4 principal components Detergents_Paper, Fresh and Delicatessen most positively correlated features and frozen is negatively coorelated.\n",
    "\n",
    "In first principal component Milk, Grocery, Detergents_Paper and Delicatessen are positiviely corelated, it means if somebody buys Detergents_Paper it strongly suggests that they also buy Milk and Grocery but they might not buy frozen and fresh since frozen and fresh is negatively corelated. We can make this kind of inference in other components as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-295d804f10dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display sample log-data after having a PCA transformation applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pca_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# Display sample log-data after having a PCA transformation applied\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Dimensionality Reduction\n",
    "When using principal component analysis, one of the main goals is to reduce the dimensionality of the data â€” in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the *cumulative explained variance ratio* is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign the results of fitting PCA in two dimensions with `good_data` to `pca`.\n",
    " - Apply a PCA transformation of `good_data` using `pca.transform`, and assign the reuslts to `reduced_data`.\n",
    " - Apply a PCA transformation of the sample log-data `log_samples` using `pca.transform`, and assign the results to `pca_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fit PCA to the good data using only two dimensions\n",
    "pca = PCA(n_components=2).fit(good_data)\n",
    "\n",
    "# TODO: Apply a PCA transformation the good data\n",
    "reduced_data = pca.transform(good_data)\n",
    "\n",
    "# TODO: Apply a PCA transformation to the sample log-data\n",
    "pca_samples = pca.transform(log_samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n",
    "print reduced_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "*What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm?\n",
    "K Means clustering* is a quick and conceptually straightforward algorithm for clustering data. It works well when the data clusters are relatively simple in shape, like Gaussian hyperspheres, but can struggle to identify clusters properly when the clusters have more complex non-linear geometries.\n",
    "\n",
    "Gaussian Mixture Models* is an generalization of K Means clustering that takes into account the covariance of the data. It os very fast, and it does not presume the data has a specific structure that may in fact not be applicable.\n",
    "(*From http://scikit-learn.org/stable/modules/)\n",
    "\n",
    "Now, lets imagine some unclustered data. K-means/Mixture of Gaussians tries to break them into clusters.\n",
    "Let's says we are aiming to break them into three clusters. K means will start with the assumption that a given data point belongs to one cluster. \n",
    "Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a cluster 1. In the next iteration, we might revise that belief, and be certain that it belongs to the cluster 2. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the \"hard assignment\".\n",
    "\n",
    "What if we are uncertain? What if we think, well, I can't be sure, but there is 70% chance it belongs to the cluster 1, but also 10% chance its in Cluster 2, 20% chance it might be Cluster 3. That's a soft assignment. The Mixture of Gaussian model helps us to express this uncertainty. It starts with some prior belief about how certain we are about each point's cluster assignments. As it goes on, it revises those beliefs. But it incorporates the degree of uncertainty we have about our assignment.\n",
    "\n",
    "    Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?\n",
    "I have manually checked both the algos in our dataset against silhouette_score and it came out K-means is performing better than GMM. \n",
    "K-means is an effective algo. to extract a given number of clusters from a training set and in our case it is doing better than GMM clustering.\n",
    "\n",
    "GMM clustering is more flexible but need not to be the more accurate than K-means because you can view it as a fuzzy or soft clustering method. Soft clustering methods assign a score to a data point for each cluster. The value of the score indicates the association strength of the data point to the cluster. As opposed to hard clustering methods, soft clustering methods are flexible in that they can assign a data point to more than one cluster. When clustering with GMMs, the score is the posterior probability. For an example of soft clustering using GMM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Creating Clusters\n",
    "Depending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known *a priori*, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data â€” if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's *silhouette coefficient*. The [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the *mean* silhouette coefficient provides for a simple scoring method of a given clustering.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Fit a clustering algorithm to the `reduced_data` and assign it to `clusterer`.\n",
    " - Predict the cluster for each data point in `reduced_data` using `clusterer.predict` and assign them to `preds`.\n",
    " - Find the cluster centers using the algorithm's respective attribute and assign them to `centers`.\n",
    " - Predict the cluster for each sample data point in `pca_samples` and assign them `sample_preds`.\n",
    " - Import sklearn.metrics.silhouette_score and calculate the silhouette score of `reduced_data` against `preds`.\n",
    "   - Assign the silhouette score to `score` and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#keep the scores for each cluster size\n",
    "sil_scores = []\n",
    "\n",
    "random_state = 7\n",
    "\n",
    "for i in range(7,1,-1):\n",
    "    clusterer = KMeans(i, random_state=random_state).fit(reduced_data)\n",
    "    # TODO: Predict the cluster for each data point\n",
    "    preds = clusterer.predict(reduced_data)\n",
    "\n",
    "    # TODO: Find the cluster centers\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    # TODO: Predict the cluster for each transformed sample data point\n",
    "    sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "    # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "    score = silhouette_score(reduced_data, preds)\n",
    "    sil_scores.append(score)\n",
    "    print i, 'clusters:', score.round(5)\n",
    "\n",
    "# plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "_ = plt.plot(np.arange(7,1,-1), sil_scores, '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "*Report the silhouette score for several cluster numbers you tried. Of these, which number of clusters has the best silhouette score?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object lies within its cluster. \n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "The silhouette score (SS) comes out to be\n",
    "7 clusters: 0.35491\n",
    "6 clusters: 0.36365\n",
    "5 clusters: 0.35319\n",
    "4 clusters: 0.33115\n",
    "3 clusters: 0.36399\n",
    "2 clusters: 0.44716\n",
    "\n",
    "The best silhouette score is for 2 clusters whihc is obvious as seperating any data points with a plane is the easiest but our underline data in question is more complex than the 2 clusters and if you read carefully on the grouping (G1, G2, G3, G4, G5 and G6) are the most logical groups based on total spending profile of individual cutstomers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Visualization\n",
    "Once you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# based on reviewer it has chosen the solution based on best silhouette score which is cluster = 2, I will not personally \n",
    "# choose this solution as the underline structure is more complex than just 2 clusters and I still \n",
    "#beliver my previous selction of 6 clusters is better solution in real life\n",
    "\n",
    "\n",
    "# Display the results of the clustering from implementation\n",
    "rs.cluster_results(reduced_data, preds, centers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Data Recovery\n",
    "Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the *averages* of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to *the average customer of that segment*. Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Apply the inverse transform to `centers` using `pca.inverse_transform` and assign the new centers to `log_centers`.\n",
    " - Apply the inverse function of `np.log` to `log_centers` using `np.exp` and assign the true centers to `true_centers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Inverse transform the centers\n",
    "\n",
    "log_centers = pca.inverse_transform(centers)\n",
    "\n",
    "# TODO: Exponentiate the centers\n",
    "true_centers = np.exp(log_centers)\n",
    "\n",
    "# Display the true centers\n",
    "segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "\n",
    "true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\n",
    "\n",
    "true_centers.index = segments\n",
    "\n",
    "true_centers['total'] = true_centers.sum(axis=1)\n",
    "\n",
    "display(true_centers)\n",
    "\n",
    "#true_centers.plot(kind = 'bar', figsize = (10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 8\n",
    "Consider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. *What set of establishments could each of the customer segments represent?*  \n",
    "**Hint:** A customer who is assigned to `'Cluster X'` should best identify with the establishments represented by the feature set of `'Segment X'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the explanation if you only choose 2 clusters as suggested by reviewer (Please note this solution is not application in real life, but I am accomodating the reviewer comments)\n",
    "\n",
    "if we condsider these 2 segments, (remember the total average spending from descriptive stat = 33226\n",
    "\n",
    "Segment 0 represents the customer who spends far below than total average spending\n",
    "Segment 1 represents the customer who spends neaerer to total average spending\n",
    "\n",
    "Reviewer suggested to explain like \"For Sample 0, the values for 'Grocery', 'Milk', and 'Detergents_Paper' are above average and match the Cluster 1 center in those categories. 'Delicatessen' is near average for both sample and the center of its predicted cluster, and 'Frozen' is below average\"\n",
    "\n",
    "In general from the delivery perspective there should be clear distintion on the  customer profile and one of the best way is to understand the toal spedning profile and behavious of each customer. For example frozen and short self life products need different delivery schedule and this condition also suggests that we can't simply have 2 groups, we should give priority to large enterprises and regular buyers and adjust the delivery schedule on tiered delivery model. A customer scoring model is another way to create a tierd model.\n",
    "\n",
    "Here another application of unsupervised learning is that we can create perfect delivery schedule based on a product mix. This kind of application is very common in supply chain and logistics area.\n",
    "\n",
    "______________________________________\n",
    "\n",
    "Below if my explanation based on if you choose cluster = 6\n",
    "If you follow my answer above I predicted before start of the problem that there is clear 6 groups.\n",
    "\n",
    "I will repeat some parts here again, if we compare with the discriptive stats\n",
    "\n",
    "If we like to dig down further on despriptive stat for field 'total' we can see 6 clear groups (G1, G2, G3, G4, G5 and G6). \n",
    "Please note (Q stands for quartile and G stands for groups), also remember that the average total spending of customers is 33226.\n",
    "\n",
    "G1 ( for field 'total' Q 25% is 17448) --> Represent very low yearly total expenditure, probably small caffe or resturant spending less than total average.\n",
    "\n",
    "G2 (for field 'total' Q 50% is 27492)  --> In between of Q 25% and Q 50% but still spending less than 'total' average\n",
    "\n",
    "G3 (for field 'total' Q 75% is 41307) --> Represent high yearly expenditure, pobably big caffe chains, resutant chaines, retailers, markets, spending above average\n",
    "\n",
    "G4 (for field 'total near minimum 904) --> Data points near the minimum, these are not year long consistent buyers it suggest they might be getting their stuff from others or they are really small buyers, this group must pay attention as this present the opportunity to grow business in this group.\n",
    "\n",
    "G5 (for field 'total' near maximum 199891) ---> These data points indicates the biggest customers as well as biggest enterprises like big retailers chains.\n",
    "\n",
    "G6 (for field 'total' near Mean 33226) --> The data points near the average and we can consider these as the year long medium buyer and also suggest medium size customers.\n",
    "\n",
    "\n",
    "Roughly the examples shows\n",
    "Segment 0 and 1 belongs to G3\n",
    "Segment 2 belongs to G4\n",
    "Segment 3 belongs to G1\n",
    "Segemnt 4 and 5 belongs to G3\n",
    "\n",
    "This gives ideas to imporve our model. It says very thin line between segment 0 & 1 ans also between 4 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 9\n",
    "*For each sample point, which customer segment from* ***Question 8*** *best represents it? Are the predictions for each sample point consistent with this?*\n",
    "\n",
    "Run the code block below to find which cluster each sample point is predicted to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the predictions\n",
    "print sample_preds\n",
    "for i, pred in enumerate(sample_preds):\n",
    "    print \"Sample point\", i, \"predicted to be in Cluster\", pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is based on reviewer comments and while choosing cluster = 2\n",
    "\n",
    "Sample point 0 and 1 belongs to cluster 1, represents the customer who spends neaerer to total average spending\n",
    "Sample point 2 belongs to cluster 0, represents the customer who spends far below than total average spending\n",
    "\n",
    "__________________________\n",
    "Below if my explanation based on if you choose cluster = 6\n",
    "As per our model sample points 0 and 1 belongs to cluster 0 and sample point 2 point belongs to cluster 5, \n",
    "\n",
    "previously I said (We have 6 groups G1, G2, G3, G4, G5 and G6)\n",
    "0 represented in  group G5 maximum category and chances of biggest enterprises like big retailers chains or large resturant chain\n",
    "1 represents in group G3 big caffe, retailer or resturant\n",
    "2 represents in group G1, low expenditure, probably small caffe or resturant\n",
    "\n",
    "It gives indication that cluster 0 represents the groups of big guys, means group of big enterprises and cluster 5 is group of small enterprises (In our graph above cluster 0 and 5 are on oppositie side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 10\n",
    "*Companies often run [A/B tests](https://en.wikipedia.org/wiki/A/B_testing) when making small changes to their products or services. If the wholesale distributor wanted to change its delivery service from 5 days a week to 3 days a week, how would you use the structure of the data to help them decide on a group of customers to test?*  \n",
    "**Hint:** Would such a change in the delivery service affect all customers equally? How could the distributor identify who it affects the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B tests, as the name implies, two versions (A and B) are compared, which are identical except for one variation that might affect a user's behavior. Version A might be the currently used version (control), while version B is modified in some respect (treatment). A/B test is very much applied to the wholesale distributor in this case where it can be compared based on the 5 days (control) and 3 data (treatment) and the result can be used to measure the satisfaction of the customers.\n",
    "\n",
    "Would such a change in the delivery service affect all customers equally?\n",
    "Now as per our model it is clear that all customers are not equal and thats why they should not treated equally. We should use the help of clusters to define the levels of custers and built a clear cut service metrices.\n",
    "\n",
    "How could the distributor identify who it affects the most?\n",
    "Well A/B test is one good idea in conjuction with the clustering model. A/B test will give us the idea which customer are not satisfied. Then we can use this information with our model and futher enhance and get the customer profiles. Also note the big thing here is that we should run separate A / B tests on each group independently.\n",
    "\n",
    "Intutively the customers who brings 80% of revenue should be treated different. A tiered delivery serive level model should be evolved based on results of this whole excerise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "*Assume the wholesale distributor wanted to predict a new feature for each customer based on the purchasing information available. How could the wholesale distributor use the structure of the clustering data you've found to assist a supervised learning analysis?*  \n",
    "**Hint:** What other input feature could the supervised learner use besides the six product features to help make a prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Assume the wholesale distributor wanted to predict a new feature for each customer based on the purchasing information available\n",
    "I think one of the good predictor is the total expenditure by each customer, I have done a quick model in other attached python book. This is one feature I will suggest for  wholesale distributor to predict. Another feature from the data is the % of spend in each product against the total expenditure for individual customers. This might build better model.\n",
    "\n",
    "    How could the wholesale distributor use the structure of the clustering data you've found to assist a supervised learning analysis?\n",
    "The clustering of data can be used in 2 ways\n",
    "1) To solve the current issue with customer complaints: identfy the unsatisfied customer using A/B test, understand the clusters of customers, it will also give the profile of customers. Create a tierd delivery service model which should able to address the cluster variance and demand of customer belongs to different clusters.\n",
    "\n",
    "2) Another way it can help from the long term perspective is the future identification of new customers into respective clusters, it can also predict real time the evoulution of current customers, if the buying pattern of the customer changes then algo can predict the cluster automatically and the service model to responds the changing needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Underlying Distributions\n",
    "\n",
    "At the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier on to the original dataset.\n",
    "\n",
    "Run the code block below to see how each data point is labeled either `'HoReCa'` (Hotel/Restaurant/Cafe) or `'Retail'` the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the clustering results based on 'Channel' data\n",
    "rs.channel_results(reduced_data, outliers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "*How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? Would you consider these classifications as consistent with your previous definition of the customer segments?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers?\n",
    "\n",
    "As per the clusters in the algo and above comparision it doesn't match. In the graph above there is no clear pattern difference between HoReCa and Retailers. It is clear from our cluster model that it need more structure and these 2 catagories are not enough to define the underline pattern. As it is defined earlier HoReCa and Retailers can be small, medium and large enterprises, thats what our clustering model is predicting\n",
    "\n",
    "    Would you consider these classifications as consistent with your previous definition of the customer segments?\n",
    "These classifcation is not consistent with the customer segments i think this is the reason why distributer is facing problem, if you consider this classification you don't have much information it is just 2 categories HoReCa and Retailers and I think distributer made big mistake by considering everybody same and decided to change the protocal for delivery service and thats where the complaints starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
